# -*- coding: utf-8 -*-
"""AI_Project_CNN_ALEXNET_GOOGLE_NET.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nsvBwo5zx7c7do3wakE_20L4JRI_wuof

**Prayer (Salat) Posture Classification and Analysis Using Convolutional Neural Network (CNN), ALEXNET and GOOGLENET Deep Learning Neural Network**

By: 

**Salman Ahmed Khan (19K-1043)
Sadeem Sattar (19K-1102)
Abdullah Tilal Khan (19K-1103)**
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import os
import matplotlib.pyplot as plt

# Images Dataset Path
posture_types = os.listdir('drive/MyDrive/Salat_Dataset')

posture = []
DIR = 'drive/MyDrive/Salat_Dataset'
for item in posture_types:
  all_posture = os.listdir(DIR + '/' + item)
  
  # Fetched folders
  for p in all_posture:
    posture.append((item, str(DIR + '/' + item) + '/' + p))

# Build dataframe
posture_df = pd.DataFrame(data=posture, columns=['posture type', 'image'])
print(posture_df.head())
print("Total number of images in the dataset: ", len(posture_df))

import cv2
path = DIR + '/'

# Resize the images to 224x224
image_size = 224

images = []
labels = []

for i in posture_types:
  data_path = path + str(i)
  filenmes = [i for i in os.listdir(data_path)]

  for f in filenmes:
    
    img = cv2.imread(data_path + '/' + f)
    try:
      img = cv2.resize(img, (image_size, image_size))
      images.append(img)
      labels.append(i)
    except:
        break

images = np.array(images)

# Normalization
images = images.astype('float32') / 255.0
print(images.shape)

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer

y = posture_df['posture type'].values

print(y.shape)

# Label Encoding on Labels
y_labelencoder = LabelEncoder()
y = y_labelencoder.fit_transform(y)

# Reshape
y = y.reshape(-1,1)

# One-Hot-Encoder on Labels
onehotencoder = ColumnTransformer([('my_ohe', OneHotEncoder(), [0])], remainder='passthrough')
Y = onehotencoder.fit_transform(y)
print(Y)
print(images.shape)

"""Train and Test DataSet"""

from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

# Shuffle images and labels
images, Y = shuffle(images, Y, random_state=1)

# Generate train and test dataset 70-30 ratio
train_x, test_x, train_y, test_y = train_test_split(images, Y, test_size=0.3, random_state=415)

#inspect the shape of the training and testing
print(train_x.shape)
print(train_y.shape)
print(test_x.shape)
print(test_y.shape)

import tensorflow as tf

train_x = np.array(train_x).reshape(-1, 224, 224, 3)

test_x = np.array(test_x).reshape(-1, 224, 224, 3)

"""**Convolutional Neural Network**"""

import tensorflow as tf

model_1 = tf.keras.models.Sequential([
                                    tf.keras.layers.Conv2D(128,(3,3),activation = 'relu', input_shape=(224,224,3)),
                                    tf.keras.layers.MaxPool2D(2,2),
                                    #####################################
                                    tf.keras.layers.Conv2D(64,(3,3),activation = 'relu'),
                                    tf.keras.layers.MaxPool2D(2,2),
                                    #####################################
                                    tf.keras.layers.Flatten(),
                                    #####################################
                                    tf.keras.layers.Dense(512, activation = 'relu'),
                                    #####################################
                                    tf.keras.layers.Dense(10, activation = 'relu'),
                                    #####################################
                                    tf.keras.layers.Dense(3, activation = 'softmax')
                                  ])
opt = tf.keras.optimizers.Adam(learning_rate=0.0001)
model_1.compile(optimizer= opt, loss='categorical_crossentropy',metrics=['accuracy'])

"""Training Model"""

history_CNN = model_1.fit(x=train_x, y=train_y, batch_size=16, epochs=50,validation_split=0.1, shuffle=True)

"""Saving Model CNN"""

model_1.save('drive/MyDrive/cnn_model.h5')

"""Reteriving Model CNN"""

import tensorflow as tf
CNN_Model_Load = tf.keras.models.load_model('drive/MyDrive/cnn_model.h5')
CNN_Model_Load.summary()

"""Evaluating Model"""

# Evaluate the model on the test data using `evaluate`
test_x = np.array(test_x).reshape(-1, 224, 224, 3)
print(test_x.shape, test_y.shape)
print("CNN_Evaluate on test data")

# CNN Model
results = model_1.evaluate(test_x, test_y)

print("test loss: ", results[0])
print("test accuracy: ", results[1])

"""**ALEXNET NEURAL NETWORK**"""

import tensorflow as tf

model_2 = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(224,224,3)),
    
    tf.keras.layers.BatchNormalization(),
    
    tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),
    
    tf.keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding="same"),
    
    tf.keras.layers.BatchNormalization(),
    
    tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),
    
    tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),
    
    tf.keras.layers.BatchNormalization(),
    
    tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),
    
    tf.keras.layers.BatchNormalization(),
    
    tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding="same"),
    
    tf.keras.layers.BatchNormalization(),
    
    tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),
    
    tf.keras.layers.Flatten(),
    
    tf.keras.layers.Dense(4096, activation='relu'),
    
    tf.keras.layers.Dropout(0.5),
    
    tf.keras.layers.Dense(4096, activation='relu'),
    
    tf.keras.layers.Dropout(0.5),
    
    tf.keras.layers.Dense(3, activation='softmax')
])

opt = tf.keras.optimizers.Adam(learning_rate=0.0001)
model_2.compile(optimizer= opt, loss='categorical_crossentropy',metrics=['accuracy'])

"""Training Model"""

history_ALEXNET = model_2.fit(x=train_x, y=train_y, batch_size=16, epochs=50,validation_split=0.1, shuffle=True)

"""Saving Model ALEXNET"""

model_2.save('drive/MyDrive/alexnet_model.h5')

"""Reteriving Model ALEXNET"""

import tensorflow as tf
ALEXNET_Model_Load = tf.keras.models.load_model('drive/MyDrive/alexnet_model.h5')
ALEXNET_Model_Load.summary()

"""Evaluating Model ALEXNET"""

# Evaluate the model on the test data using `evaluate`
test_x = np.array(test_x).reshape(-1, 224, 224, 3)
print(test_x.shape, test_y.shape)
print("ALEXNET_Evaluate on test data")

# ALEXNET Model
results = model_2.evaluate(test_x, test_y)

print("test loss: ", results[0])
print("test accuracy: ", results[1])

"""**GOOGLENET NEURAL NETWORK**"""

from keras.models import Model
from keras.layers import Input, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, GlobalAveragePooling2D, Dense, Dropout
from keras.layers.merge import concatenate

"""Inception Block"""

def Inception_block(input_layer, f1, f2_conv1, f2_conv3, f3_conv1, f3_conv5, f4): 
  # Input: 
  # - f1: number of filters of the 1x1 convolutional layer in the first path
  # - f2_conv1, f2_conv3 are number of filters corresponding to the 1x1 and 3x3 convolutional layers in the second path
  # - f3_conv1, f3_conv5 are the number of filters corresponding to the 1x1 and 5x5  convolutional layer in the third path
  # - f4: number of filters of the 1x1 convolutional layer in the fourth path

  # 1st path:
  path1 = Conv2D(filters=f1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)

  # 2nd path
  path2 = Conv2D(filters = f2_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)
  path2 = Conv2D(filters = f2_conv3, kernel_size = (3,3), padding = 'same', activation = 'relu')(path2)

  # 3rd path
  path3 = Conv2D(filters = f3_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)
  path3 = Conv2D(filters = f3_conv5, kernel_size = (5,5), padding = 'same', activation = 'relu')(path3)

  # 4th path
  path4 = MaxPooling2D((3,3), strides= (1,1), padding = 'same')(input_layer)
  path4 = Conv2D(filters = f4, kernel_size = (1,1), padding = 'same', activation = 'relu')(path4)

  output_layer = concatenate([path1, path2, path3, path4], axis = -1)

  return output_layer

"""GOOGLENET ALGORITHM"""

def GoogLeNet():
  # input layer 
  input_layer = Input(shape = (224, 224, 3))

  # convolutional layer: filters = 64, kernel_size = (7,7), strides = 2
  X = Conv2D(filters = 64, kernel_size = (7,7), strides = 2, padding = 'valid', activation = 'relu')(input_layer)

  # max-pooling layer: pool_size = (3,3), strides = 2
  X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)

  # convolutional layer: filters = 64, strides = 1
  X = Conv2D(filters = 64, kernel_size = (1,1), strides = 1, padding = 'same', activation = 'relu')(X)

  # convolutional layer: filters = 192, kernel_size = (3,3)
  X = Conv2D(filters = 192, kernel_size = (3,3), padding = 'same', activation = 'relu')(X)

  # max-pooling layer: pool_size = (3,3), strides = 2
  X = MaxPooling2D(pool_size= (3,3), strides = 2)(X)

  # 1st Inception block
  X = Inception_block(X, f1 = 64, f2_conv1 = 96, f2_conv3 = 128, f3_conv1 = 16, f3_conv5 = 32, f4 = 32)

  # 2nd Inception block
  X = Inception_block(X, f1 = 128, f2_conv1 = 128, f2_conv3 = 192, f3_conv1 = 32, f3_conv5 = 96, f4 = 64)

  # max-pooling layer: pool_size = (3,3), strides = 2
  X = MaxPooling2D(pool_size= (3,3), strides = 2)(X)

  # 3rd Inception block
  X = Inception_block(X, f1 = 192, f2_conv1 = 96, f2_conv3 = 208, f3_conv1 = 16, f3_conv5 = 48, f4 = 64)

  # Extra network 1:
  X1 = AveragePooling2D(pool_size = (5,5), strides = 3)(X)
  X1 = Conv2D(filters = 128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X1)
  X1 = Flatten()(X1)
  X1 = Dense(1024, activation = 'relu')(X1)
  X1 = Dropout(0.7)(X1)
  X1 = Dense(3, activation = 'softmax')(X1)

  
  # 4th Inception block
  X = Inception_block(X, f1 = 160, f2_conv1 = 112, f2_conv3 = 224, f3_conv1 = 24, f3_conv5 = 64, f4 = 64)

  # 5th Inception block
  X = Inception_block(X, f1 = 128, f2_conv1 = 128, f2_conv3 = 256, f3_conv1 = 24, f3_conv5 = 64, f4 = 64)

  # 6th Inception block
  X = Inception_block(X, f1 = 112, f2_conv1 = 144, f2_conv3 = 288, f3_conv1 = 32, f3_conv5 = 64, f4 = 64)

  # Extra network 2:
  X2 = AveragePooling2D(pool_size = (5,5), strides = 3)(X)
  X2 = Conv2D(filters = 128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X2)
  X2 = Flatten()(X2)
  X2 = Dense(1024, activation = 'relu')(X2)
  X2 = Dropout(0.7)(X2)
  X2 = Dense(3, activation = 'softmax')(X2)
  
  
  # 7th Inception block
  X = Inception_block(X, f1 = 256, f2_conv1 = 160, f2_conv3 = 320, f3_conv1 = 32, 
                      f3_conv5 = 128, f4 = 128)

  # max-pooling layer: pool_size = (3,3), strides = 2
  X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)

  # 8th Inception block
  X = Inception_block(X, f1 = 256, f2_conv1 = 160, f2_conv3 = 320, f3_conv1 = 32, f3_conv5 = 128, f4 = 128)

  # 9th Inception block
  X = Inception_block(X, f1 = 384, f2_conv1 = 192, f2_conv3 = 384, f3_conv1 = 48, f3_conv5 = 128, f4 = 128)

  # Global Average pooling layer 
  X = GlobalAveragePooling2D(name = 'GAPL')(X)

  # Dropoutlayer 
  X = Dropout(0.4)(X)

  # output layer 
  X = Dense(3, activation = 'softmax')(X)
  
  # model
  model = Model(input_layer, [X, X1, X2], name = 'GoogLeNet')

  return model

"""Model Compile"""

model_3 = GoogLeNet()

opt = tf.keras.optimizers.Adam(learning_rate=0.0001)
model_3.compile(optimizer= opt, loss='categorical_crossentropy',metrics=['accuracy'])

"""Training Model GOOGLENET"""

history_GOOGLENET = model_3.fit(x=train_x, y=train_y, batch_size=16, epochs=50,validation_split=0.1, shuffle=True)

"""Saving Model GOOGLENET"""

model_3.save('drive/MyDrive/googlenet_model.h5')

"""Reteriving the Model"""

import tensorflow as tf
GOOGLENET_Model_Load = tf.keras.models.load_model('drive/MyDrive/googlenet_model.h5')
GOOGLENET_Model_Load.summary()

"""Evaluating Model GOOGLENET"""

# Evaluate the model on the test data using `evaluate`
test_x = np.array(test_x).reshape(-1, 224, 224, 3)
print(test_x.shape, test_y.shape)
print("GOOGLENET_Evaluate on test data")

# GOOGLENET Model
results = model_3.evaluate(test_x, test_y)

print("test loss: ", results[0])
print("test accuracy: ", results[1])

"""**Plotting The Graph**"""

# Plotting Loss
plt.plot(history_CNN.history['loss'])
plt.plot(history_ALEXNET.history['loss'])
plt.plot(history_GOOGLENET.history['loss'])

# Plotting Accuracy
plt.plot(history_CNN.history['accuracy'])
plt.plot(history_ALEXNET.history['accuracy'])
plt.plot(history_GOOGLENET.history['dense_13_accuracy'])

# Legend
plt.legend(['CNN_loss', 'ALEXNET_loss', 'GOOGLENET_loss', 'CNN_accuracy', 'ALEXNET_accuracy', 'GOOGLENET_accuracy'])

"""**PREDICTION**"""

# CNN Prediction
CNN_pred = model_1.predict(test_x) 

CNN_pred = np.argmax(CNN_pred, axis = 1) 
label = np.argmax(test_y,axis = 1) 
print("CNN Model Prediction\n")
print(CNN_pred) 
print(label)

# ALEXNET Prediction
ALEXNET_pred = model_2.predict(test_x) 

ALEXNET_pred = np.argmax(ALEXNET_pred, axis = 1) 
label = np.argmax(test_y,axis = 1) 
print("\nALEXNET Model Prediction\n")
print(ALEXNET_pred) 
print(label)

# GOOGLENET Prediction
GOOGLENET_pred = model_3.predict(test_x) 

GOOGLENET_pred = np.argmax(GOOGLENET_pred, axis = 1) 
label = np.argmax(test_y,axis = 1) 
print("\nGOOGLENET Model Prediction\n")
print(GOOGLENET_pred) 
print(label)